{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise sheet is split in two parts.\n",
    "- The first part is about regression and regularization on toy data.\n",
    "- The second part is about disease classification on a medical dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Regression and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "\n",
    "Your task is to implement different regression models. Please use the functions in the following cells to implement\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Polynomial Regression\n",
    "- Polynomial Ridge Regression\n",
    "\n",
    "You can assume the input data to have the following shapes\n",
    "\n",
    "- X: (num_features, num_samples)\n",
    "- y: (num_targets, num_samples)\n",
    "\n",
    "Your output should be the weight vector ```w``` have the following shape\n",
    "\n",
    "- w: (num_features, 1)\n",
    "\n",
    "Hint for polynomial regression: think about how you can reduce polynomial regression to linear regression. You may use the previously implemented functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y, lam):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(X, y, m):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_ridge_regression(X, y, m, lam):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data sets\n",
    "\n",
    "Here, we generate 4 artifical datasets to apply your models to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array(np.linspace(-3,3,100).reshape(1,100))\n",
    "y1 = 3*X1 + 1*rng.randn(100)\n",
    "\n",
    "X2 = np.array(np.linspace(-20,20, 500).reshape(1,500))\n",
    "y2 = (5 * (X2)**2) + 200*np.sin(X2) + 100*rng.randn(500)\n",
    "\n",
    "X3 = np.array(np.linspace(0,np.pi*4,200).reshape(1,200))\n",
    "y3 = 10*np.sin(X3) + 5*rng.randn(200) + 3*X3\n",
    "\n",
    "X4 = np.linspace(0,4,200).reshape(1,200)\n",
    "y4 = -30*X4 + 5*rng.randn(200)\n",
    "\n",
    "num_outlier = 30\n",
    "outlier4 = np.linspace(4,4.2, num_outlier).reshape(1,num_outlier)\n",
    "X4 = np.hstack([X4, outlier4])\n",
    "y4 = np.hstack([y4, outlier4+ rng.randn(num_outlier)])\n",
    "\n",
    "data_sets = [(X1, y1), (X2, y2), (X3, y3), (X4, y4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "Please apply the linear regression model on all data sets in `data_sets` and visualize the results by\n",
    "-  plot the data as points\n",
    "-  plot the regressed line.\n",
    "\n",
    "What happens if you apply ridge regression with different lambdas instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "axes =[ax1, ax2, ax3, ax4]\n",
    "                                         \n",
    "for idata_set, data_set in enumerate(data_sets):\n",
    "    X,y = data_set\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE\n",
    "    axes[idata_set].plot(X.T, y.T, \"o\", c=\"b\")\n",
    "    axes[idata_set].plot(X.T, y_hat, c=\"r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "Please apply polynomial regression on the second data set with varying polynomial degree `D` and interpret the solution. Write your predictions to ```y_hat```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X,y = data_sets[1]\n",
    "Ds = np.arange(2,20,1)\n",
    "colors = plt.cm.viridis(Ds/(Ds.max()+1))\n",
    "\n",
    "y_hats = []\n",
    "\n",
    "# START YOUR CODE\n",
    "for D in Ds:\n",
    "    continue\n",
    "# END YOUR CODE\n",
    "\n",
    "Z = [[0,0],[0,0]]\n",
    "CS3 = plt.contourf(Z, Ds, cmap=plt.cm.viridis)\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "    \n",
    "plt.plot(X.T, y.T, \"o\", c=\"b\", alpha=.5)\n",
    "for iline,line in enumerate(y_hats):\n",
    "    plt.plot(X.T, line, c=colors[iline], alpha=1, linewidth=3)\n",
    "\n",
    "_ = plt.colorbar(CS3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4\n",
    "\n",
    "Please apply polynomial ridge regression on the third data set with varying $\\lambda$ and interpret the solution. Write your predictions to ```y_hat```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as clr\n",
    "D = 7\n",
    "\n",
    "\n",
    "X,y = data_sets[2]\n",
    "lams = np.arange(0, 100, .5)\n",
    "\n",
    "colors = plt.cm.viridis(lams/100)\n",
    "\n",
    "y_hat = []\n",
    "\n",
    "# START YOUR CODE\n",
    "for lam in lams:    \n",
    "    continue\n",
    "# END YOUR CODE\n",
    "\n",
    "Z = [[0,0],[0,0]]\n",
    "CS3 = plt.contourf(Z, lams, cmap=plt.cm.viridis)\n",
    "plt.clf()\n",
    "    \n",
    "plt.plot(X.T, y.T, \"o\", c=\"b\")\n",
    "for iline,line in enumerate(y_hat):\n",
    "    plt.plot(X.T, line, c=colors[iline], alpha=.5)\n",
    "\n",
    "_ = plt.colorbar(CS3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Classification: Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train classical ML models for classification on the publicly available \"Heart Disease Dataset\". A description of the dataset can be found here:\n",
    "\n",
    "https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
    "\n",
    "We are trying to predict if a patient has a heart disease (0 = no disease and 1 = disease) from the 13 available features.\n",
    "\n",
    "Instead of implementing methods ourselves, we use the popular ```scikit-learn``` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Reading the data. Notice that the shape of the data is (num_samples, num_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"heart.csv\")\n",
    "X = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,-1].values\n",
    "\n",
    "print(\"Data shape:\\t\", X.shape)\n",
    "print(\"Label shape:\\t\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Number of train samples:\\t', len(X_train))\n",
    "print('Number of test samples: \\t', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print('Scaled average mean:\\t', X_train.mean(axis=0).mean())\n",
    "print('Scaled average std:\\t', X_train.std(axis=0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "\n",
    "Please train a logistic regression model on the train data, and evaluate it's performance on the train and test data.\n",
    "\n",
    "You may use the sklearn implementation of the logistic regression model. The documentation can be found here: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "\n",
    "You may also use the evaluation code provided below. For that, store your predictions in variables named ```y_train_pred``` and ```y_test_pred```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# START YOUR CODE\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Train accuracy:\\t', accuracy_score(y_train, y_train_pred))\n",
    "print('Test accuracy:\\t', accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "\n",
    "See if you can find a better classifier for these data. What accruacy score can you reach? You may use any classifier implemented in the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START YOUR CODE\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train accuracy:\\t', accuracy_score(y_train, y_train_pred))\n",
    "print('test accuracy:\\t', accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "disp.plot(ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
