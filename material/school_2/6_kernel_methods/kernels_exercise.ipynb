{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIFOLD Aqtivate workshop - Kernel methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import scipy as sp\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import solve\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ols(X_train, Y_train):\n",
    "    ''' Trains ordinary least squares (ols) regression \n",
    "    Input:       X_train  -  DxN array of N data points with D features\n",
    "                 Y        -  D2xN array of length N with D2 multiple labels\n",
    "    Output:      W        -  DxD2 array, linear mapping used to estimate labels \n",
    "                             with np.dot(W.T, X)                      \n",
    "    '''\n",
    "    W = solve(X_train @ X_train.T, X_train @Y_train.T)\n",
    "    return W\n",
    "    \n",
    "def apply_ols(W, X_test):\n",
    "    ''' Applys ordinary least squares (ols) regression \n",
    "    Input:       X_test    -  DxN array of N data points with D features\n",
    "                 W        -  DxD2 array, linear mapping used to estimate labels \n",
    "                             trained with train_ols                   \n",
    "    Output:     Y_test    -  D2xN array\n",
    "    '''\n",
    "    Y_test = W.T @ X_test\n",
    "    return Y_test\n",
    "\n",
    "def test_sine_toydata(kwidth = 1, llambda = 1):\n",
    "    #Data generation\n",
    "    X_train = np.arange(0,10,.01)\n",
    "    X_train = X_train[None,:]\n",
    "    X_test = np.linspace(0, 10, 500)\n",
    "    X_test = X_test[None, :]\n",
    "    Y_train = np.sin(X_train) + np.random.normal(0, .5, X_train.shape)\n",
    "    # Linear Regression \n",
    "    w_est = train_ols(X_train, Y_train) \n",
    "    Y_est_lin = apply_ols(w_est, X_test)\n",
    "    \n",
    "    # Kernel Ridge Regression\n",
    "    alphas = train_krr(X_train, Y_train, kwidth, llambda)\n",
    "    Y_est_krr = apply_krr(alphas, X_train, X_test, kwidth)\n",
    "    \n",
    "    #Plot result\n",
    "    pl.figure()\n",
    "    pl.plot(X_train.T, Y_train.T, '+k', label = 'Train Data')\n",
    "    pl.plot(X_test.T, Y_est_lin.T, '-.', linewidth = 2, label = 'OLS')\n",
    "    pl.plot(X_test.T, Y_est_krr.T,  'r', linewidth = 2, label = 'KRR')\n",
    "    pl.xlabel('x')\n",
    "    pl.ylabel('y')\n",
    "    pl.title(r'$\\lambda$ = ' + str(llambda) + '   $\\sigma$ = ' + str(kwidth))\n",
    "    pl.legend(loc = 'lower right')\n",
    "\n",
    "\n",
    "def test_gaussian_kernel():\n",
    "    #Data generation\n",
    "    X1 = np.linspace(0, 10, 10)\n",
    "    X1 = X1[None,:]\n",
    "    X2 = np.linspace(0, 10, 20)\n",
    "    X2 = X2[None,:]\n",
    "    K = GaussianKernel(X1, X2, kwidth=1.)\n",
    "    assert K.shape == (10, 20), f'Shape is wrong. {K.shape} != (10, 20)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GaussianKernel(X1, X2, kwidth):\n",
    "    ''' Compute Gaussian Kernel \n",
    "    Input: X1    - DxN1 array of N1 data points with D features \n",
    "           X2    - DxN2 array of N2 data points with D features \n",
    "           kwidth - Kernel width\n",
    "    Output K     - N1 x N2 Kernel matrix\n",
    "    '''\n",
    "    assert(X1.shape[0] == X2.shape[0])\n",
    "    # YOUR CODE HERE\n",
    "    K = None\n",
    "    return K\n",
    "\n",
    "def train_krr(X_train, Y_train, kwidth, llambda):\n",
    "    ''' Trains kernel ridge regression (krr)\n",
    "    Input:       X_train  -  DxN array of N data points with D features\n",
    "                 Y        -  D2xN array of length N with D2 multiple labels\n",
    "                 kwdith   -  kernel width\n",
    "                 llambda    -  regularization parameter\n",
    "    Output:      alphas   -  NxD2 array, weighting of training data used for apply_krr                     \n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    alphas = None\n",
    "    return alphas\n",
    "\n",
    "\n",
    "def apply_krr(alphas, X_train, X_test, kwidth):\n",
    "    ''' Applys kernel ridge regression (krr)\n",
    "    Input:      alphas      -  NtrxD2 array trained in train_krr      \n",
    "                X_train     -  DxNtr array of Ntr train data points with D features\n",
    "                X_test      -  DxNte array of Nte test data points with D features\n",
    "                kwidht      -  Kernel width             \n",
    "    Output:     Y_test      -  D2xNte array\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    Y_test = None\n",
    "    return Y_test\n",
    "\n",
    "test_gaussian_kernel()\n",
    "test_sine_toydata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Kernel Ridge Regression (KRR)** by completing the function stubs  ```krr_train``` and  ```krr_apply```. \n",
    "Remember the notation, \n",
    "$$X_{\\text{train}} \\in \\mathbb{R}^{D_X \\times N_{tr}}, \\;  Y_{\\text{train}} \\in \\mathbb{R}^{D_Y \\times N_{tr}}, \\; X_{\\text{test}} \\in \\mathbb{R}^{D_X \\times N_{te}}$$\n",
    "In ```krr_train```, you estimate a linear combination of the input vectors $\\alpha$, \n",
    "$$\\alpha = (K + \\lambda I)^{-1}Y_{\\text{train}}^T$$\n",
    "where $\\lambda$ is the regularization parameter and $K$ is the $N_{tr} \\times N_{tr}$ Gaussian Kernel matrix with Kernel width $\\sigma$, $K_{ij} =  \\exp\\left( - \\frac{\\| X_{\\text{train}}^i - X_{\\text{train}}^j\\|^2}{\\sigma^2} \\right)$. You can compute $K$ with the provided function ```GaussianKernel```.\n",
    "\n",
    "The function ```krr_apply``` than uses the weights $\\alpha$ to predict (unknown) new test data $X_{\\text{test}}$\n",
    "$$Y_{\\text{test}} = (\\mathbf{k} \\alpha)^T.$$\n",
    "where $\\mathbf k$ is the $N_{\\text{test}} \\times N_{\\text{train}}$ matrix $\\mathbf{k}_{ij} =   \\exp\\left( - \\frac{\\| X_{\\text{test}}^i - X_{\\text{train}}^j\\|^2}{\\sigma^2}\\right)$.    \n",
    "\n",
    "We test our code on the function  ```test_sine_toydata```. It generates toy data that follows a sine function, $x_i \\in \\{0, 0.01, 0.02, \\ldots, 10\\}, y_i = \\sin(x_i) + \\epsilon, \\epsilon \\sim \\mathcal{N}(0, 0.5)$. The result of KRR should resemble the sine function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter dependence** \n",
    "We want to analyse how the Kernel Ridge solution depends on its hyperparameters, the kernel width $\\sigma$ and  the regularization parameter $\\lambda$. \n",
    "- Call the function  ```test_sine_toydata```  with $\\lambda$ = 1 for three different Kernel widths, $\\sigma = \\{0.001, 0.1, 1, 10\\}$.\n",
    "- Call the function  ```test_sine_toydata```  with $\\sigma$ = 1 for three different regularization parameters, $\\lambda = \\{ 10^{-10}, 1, 500\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on a real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "print(f'data.shape = {diabetes.data.shape}\\n'\n",
    "      f'target.shape = {diabetes.target.shape}')\n",
    "\n",
    "# create train/test split\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "(i) Train kernel ridge regression model and a linear model on this dataset and report the `mean_squared_error`.\n",
    "\n",
    "(ii) Find the optimal value for the width $\\sigma$ of the gaussian kernel.\n",
    "\n",
    "(iii) Ananlyze the spectral properties depending $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression \n",
    "# YOUR CODE HERE\n",
    "y_test_lin = None\n",
    "\n",
    "# Kernel Ridge Regression\n",
    "# YOUR CODE HERE\n",
    "y_test_krr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "mse_krr = None\n",
    "mse_lin = None\n",
    "\n",
    "print(f'mse_krr = {mse_krr}\\n'\n",
    "      f'mse_lin = {mse_lin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llambda = 10.\n",
    "# search for the optimal width sigma\n",
    "sigma_opt = None\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernel(K):\n",
    "    plt.title('Kernel matrix')\n",
    "    ax = plt.imshow(K, vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "\n",
    "def plot_singular_values(K, label=''):\n",
    "    U, s, V = np.linalg.svd(K)\n",
    "    plt.plot(s, label=label)\n",
    "    plt.semilogy()\n",
    "    plt.title('Singular values')\n",
    "    plt.ylabel('Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze kernel matrix depending on sigma\n",
    "# YOUR CODE HERE\n",
    "K = None\n",
    "plot_kernel(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_singular_values(K, label=None)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
