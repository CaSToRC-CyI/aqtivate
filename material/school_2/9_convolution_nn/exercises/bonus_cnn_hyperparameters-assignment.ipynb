{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Hyperparameters\n",
    "In this notebook, you will observe the effect of various hyperparameters on the training of a deep convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# data_root = \"/home/space/datasets/mnist\"\n",
    "data_root = \"./data/MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "num_workers, pin_memory = 4, False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # NVIDIA GPU\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon (Metal)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # CPU (slowest option)\n",
    "\n",
    "print(f\"Device set to {device}!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define network architecture\n",
    "\n",
    "Implement the `__init__` and `forward` methods of the class `Net` with the same architecture as in *cnn_example.ipynb*. Add a dropout layer to the model after the second ConvLayer using **`nn.Dropout2d`** . The dropout ratio can be specified when creating the network and it should be a member variable of the class `Net`, i.e. it can be accessed by `self.dropout_ratio` inside the class. *See cnn_example.ipynb as a reference*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Dropout2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ee249619476d93ebca34fbb96cb65d5c",
     "grade": false,
     "grade_id": "cell-7f5987a986233b93",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line by your code.\")\n",
    "    # <<<<< END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and model setup\n",
    "Implement a function `setup` that builds the data loader as well as the model and optimizer. All relevant hyperparameters parameters are parsed as optional arguments to the function. *See cnn_example.ipynb as a reference*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4237f20058fa86ab3349eb59865b0510",
     "grade": false,
     "grade_id": "cell-834a5dcd9d4cf86d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def setup(dropout_ratio=0.5, lr=1e-2, momentum=0.5, batch_size=128, mean=0.1307, std=0.3081):\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line by your code.\")\n",
    "    # <<<<< END YOUR CODE\n",
    "    return train_loader, device, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Below you are given a function that performs the training for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    losses = list()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the data loader, model and optimizer, run the training (e.g. for 3 epochs) and plot the evolution of the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "38ba12bcd1634ad0519bd973ad86c748",
     "grade": false,
     "grade_id": "cell-3980229d6ed44a36",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# >>>>> YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line by your code.\")\n",
    "# <<<<< END YOUR CODE\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Try out several values (given below) of the following hyperparameters:\n",
    "* Learning rate\n",
    "* Momentum\n",
    "* Batch size\n",
    "\n",
    "Visualize the learning curves in one plot for comparison.\n",
    "\n",
    "*Optional*: Investigate the result if you assume that the data are already normalized (change the mean and standard deviation accordingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrs = [1e0, 1e-2, 1e-4]\n",
    "momenta = [0.0, 0.5, 0.9]\n",
    "batch_sizes = [32, 128, 1024]  # CAVE: compare epochs, not iterations\n",
    "dropout_ratios = [0.0, 0.5, 0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "772308125e249e6d64173ba0cadd9a65",
     "grade": false,
     "grade_id": "cell-d8d5e0d86fd5ed31",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for lr in lrs:\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line by your code.\")\n",
    "    # <<<<< END YOUR CODE\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "332aaac30a2e3f18f98b45a13bf40a67",
     "grade": false,
     "grade_id": "cell-72fa94335507ee5f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for momentum in momenta:\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line by your code.\")\n",
    "    # <<<<< END YOUR CODE\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "1de0920c07c5f35d589dd71f984e10bd",
     "grade": false,
     "grade_id": "cell-57c1c9eb9a04e129",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Make sure you compare the losses for the same number of samples seen (not the number of iterations of gradient descent).\n",
    "\n",
    "plt.figure()\n",
    "for batch_size in batch_sizes:\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line by your code.\")\n",
    "    # <<<<< END YOUR CODE\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of samples seen\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization error\n",
    "Even though the training loss can give valuable hints on the hyperparameters, it is typically not what we are interested in. Much more important is the performance of the model on unseen data, the so called validation/test data. Implement a data loader `test_loader` for the test data (similar to the training data loader but set the `train` parameter of `datasets.MNIST` to `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a2622b96ed76099b31cf8c2218798d2e",
     "grade": false,
     "grade_id": "cell-ed19ee43596bf69f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# >>>>> YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line by your code.\")\n",
    "# <<<<< END YOUR CODE\n",
    "print(f\"Evaluating on {len(test_loader.dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an evaluation function that runs and evaluates the model. Compute the loss **and** the accuracy on the evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "09b3305945484df11c1836750f0f801a",
     "grade": false,
     "grade_id": "cell-fd4ee4da0a9a4386",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, device, data_loader):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Tell the model that we do not need gradient computation for evaluation\n",
    "        for data, target in data_loader:\n",
    "            # Compute output of network\n",
    "            # >>>>> YOUR CODE HERE\n",
    "            raise NotImplementedError(\"Replace this line by your code.\")\n",
    "            # <<<<< END YOUR CODE\n",
    "            \n",
    "            # Compute loss and store in list\n",
    "            # >>>>> YOUR CODE HERE\n",
    "            raise NotImplementedError(\"Replace this line by your code.\")\n",
    "            # <<<<< END YOUR CODE\n",
    "            \n",
    "            \n",
    "            prediction = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "            num_samples += len(data)\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    # >>>>> YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line by your code.\")\n",
    "    # <<<<< END YOUR CODE\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance at chance level\n",
    "Verify your evaluation function by running it on an untrained model on both the training and the test set. You can use the `setup` function from above to get a randomly initialized network or by calling the constructor of the `Net` class.\n",
    "\n",
    "Which values do you expect for the test loss and accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "0cb2e8443134b5059a6b0dc58598552a",
     "grade": false,
     "grade_id": "cell-08f7ea2ea695abe3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# >>>>> YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line by your code.\")\n",
    "# <<<<< END YOUR CODE\n",
    "\n",
    "print(f\"Training set: \\n \\t Average loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.0f}%\")\n",
    "print(f\"Test set: \\n \\t Average loss: {avg_test_loss:.4f}, Accuracy: {test_accuracy:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on trained model\n",
    "Evaluate a trained model (training procedure is provided below) on both the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader, device, model, optimizer = setup()\n",
    "\n",
    "losses = list()\n",
    "for epoch in tqdm(range(10)):\n",
    "    epoch_losses = train_epoch(model, device, train_loader, optimizer)\n",
    "    losses.extend(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2d0a14a1b0cf1a725bbe695764834b94",
     "grade": false,
     "grade_id": "cell-a34fe31ed3eb447a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# >>>>> YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line by your code.\")\n",
    "# <<<<< END YOUR CODE\n",
    "\n",
    "print(f\"Training set: \\n \\t Average loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.0f}%\")\n",
    "print(f\"Test set: \\n \\t Average loss: {avg_test_loss:.4f}, Accuracy: {test_accuracy:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of test performance over training\n",
    "Below you are given code to track and plot the test accuracy for different epochs during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader, device, model, optimizer = setup(momentum=0.9)\n",
    "\n",
    "train_losses = list()\n",
    "test_losses = list()\n",
    "train_accuracies = list()\n",
    "test_accuracies = list()\n",
    "val_epochs = list()\n",
    "for epoch in tqdm(range(8)):\n",
    "    # Training\n",
    "    epoch_losses = train_epoch(model, device, train_loader, optimizer)\n",
    "    train_losses.append(np.mean(epoch_losses))\n",
    "    \n",
    "    # Evaluation (only every other epoch)\n",
    "    if epoch % 2 == 0:\n",
    "        avg_test_losses, test_accuracy = evaluate(model, device, test_loader)\n",
    "    \n",
    "        test_losses.append(avg_test_losses)\n",
    "        test_accuracies.append(test_accuracy) \n",
    "        val_epochs.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(train_losses, color=\"b\", label=\"Train\")\n",
    "ax1.plot(val_epochs, test_losses, color=\"r\", label=\"Test\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('* Accuracy')\n",
    "ax2.plot(val_epochs, test_accuracies, color=\"r\", marker=\"*\")\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
