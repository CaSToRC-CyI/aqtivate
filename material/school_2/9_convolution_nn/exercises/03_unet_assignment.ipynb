{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation with U-Net\n",
    "\n",
    "[Saeed Salehi]\n",
    "\n",
    "We aim to implement a U-Net like model that takes an image of 4 digits and segments odd and even numbers to separate maps.\n",
    "\n",
    "Reference: [U-Net paper, Ronneberger et al. 2015](https://arxiv.org/abs/1505.04597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # header\n",
    "import sys\n",
    "sys.path.append(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please define the data directory\n",
    "data_root = \"./data\"\n",
    "assert os.path.exists(data_root), \"Data directory not found!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before going further, we need to choose the hardware to train our model on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "num_workers, pin_memory = 4, False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # NVIDIA GPU\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon (Metal)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # CPU (slowest option)\n",
    "\n",
    "print(f\"Device set to {device}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return the number of learble parameters in a model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We use MNIST dataset to compose our own data which includes 4 digits per image.\n",
    "\n",
    "**NOTE:** Our input data is the composition of four digits and our target output is the mask of odd and even digits. Therefore our label is not the class but the target mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MNIST(root=data_root,  # location for the data to be downloaded if not found\n",
    "                train=True,  # use the training set (if false, it will use the test set)\n",
    "                transform=torchvision.transforms.ToTensor(),  # transform the data to torch tensors\n",
    "                download=True,  # download the data if not found, \n",
    ")\n",
    "\n",
    "# randomly splitting the dataset into 90% training and 10% validation\n",
    "n_train = int(0.9 * len(train_ds))\n",
    "n_val = len(train_ds) - n_train\n",
    "train_ds, val_ds = torch.utils.data.random_split(train_ds, [n_train, n_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OddEvenDS(Dataset):\n",
    "    def __init__(self, ds_: Dataset):\n",
    "        self.ds = ds_  # the vanila dataset\n",
    "        self.h, self.w = 64, 64  # the new height and width\n",
    "        self.noise = 0.25  # the noise level\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Pad((2, 2, 2, 2)),\n",
    "            torchvision.transforms.RandomRotation(15),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        m = torch.zeros(2, self.h, self.w)  # segmentation masks for the two classes\n",
    "        c = torch.zeros(1, self.h, self.w)  # composite image for the digits\n",
    "\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                random_idx = torch.randint(0, len(self.ds), (1,)).item()\n",
    "                x, y = self.ds.__getitem__(random_idx)\n",
    "                x = self.transform(x)\n",
    "                c[:, i*32:(i+1)*32, j*32:(j+1)*32] = x\n",
    "                m[y%2, i*32:(i+1)*32, j*32:(j+1)*32] = x[0]\n",
    "        \n",
    "        c += self.noise * torch.randn_like(c)\n",
    "        c = c.clamp(0.0, 1.0)\n",
    "\n",
    "        return c, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_seg = OddEvenDS(train_ds)\n",
    "valid_ds_seg = OddEvenDS(val_ds)\n",
    "\n",
    "# plotting the first 5 images and their masks\n",
    "fig, axs = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i in range(5):\n",
    "    c, m = train_ds_seg[i]\n",
    "    axs[0, i].imshow(c[0], cmap=\"gray\")\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(m[0] - m[1], cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "    axs[1, i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Given the simplicity of our task, we use a simple and small U-Net model. The model has an encoder, a decoder, and 3 skip connections. The feature maps from encoder are concatenated to the corresponding decoder feature maps. Your task is to complete the encoder.\n",
    "\n",
    "**NOTE:** All the `Conv2d` layers (the red arrows) should have `stride=1, padding='same'`\n",
    "\n",
    "![Model Architecture](figures/unet_1.png)\n",
    "\n",
    "Figure adopted from U-Net paper, Ronneberger et al. 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # input shape = (1, 64, 64)\n",
    "        # # Encoder # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        # >>>>> YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Replace this line by your code.\")\n",
    "        self.conv1enc = ...  # output shape = (4, 64, 64)\n",
    "        self.conv2enc = ...  # output shape = (4, 64, 64)\n",
    "        self.maxpool1 = ...  # output shape = (4, 32, 32)\n",
    "\n",
    "        self.conv3enc = ...  # output shape = (8, 32, 32)\n",
    "        self.conv4enc = ...  # output shape = (8, 32, 32)\n",
    "        self.maxpool2 = ...  # output shape = (8, 16, 16)\n",
    "\n",
    "        self.conv5enc = ...  # output shape = (16, 16, 16)\n",
    "        self.conv6enc = ...  # output shape = (16, 16, 16)\n",
    "        self.maxpool3 = ...  # output shape = (16, 8, 8)\n",
    "\n",
    "        self.conv7enc = ...  # output shape = (32, 8, 8)\n",
    "        self.conv8enc = ...  # output shape = (32, 8, 8)\n",
    "        # <<<<< END YOUR CODE\n",
    "        # # End of Encoder # # # # # # # # # # # # # # # # # # # # \n",
    "\n",
    "        # # Decoder # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "        self.upsamp1 = nn.Upsample(size=(16, 16))\n",
    "        # output shape = (32, 16, 16)\n",
    "        self.upconv1 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        # output shape = (16, 16, 16)\n",
    "\n",
    "        self.conv1dec = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
    "        # output shape = (16, 16, 16)\n",
    "        self.conv2dec = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
    "        # output shape = (16, 16, 16)\n",
    "\n",
    "        self.upsamp2 = nn.Upsample(size=(32, 32))\n",
    "        # output shape = (16, 32, 32)\n",
    "        self.upconv2 = nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        # output shape = (8, 32, 32)\n",
    "\n",
    "        self.conv3dec = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "        # output shape = (8, 32, 32)\n",
    "        self.conv4dec = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "        # output shape = (8, 32, 32)\n",
    "\n",
    "        self.upsamp3 = nn.Upsample(size=(64, 64))\n",
    "        # output shape = (8, 64, 64)\n",
    "        self.upconv3 = nn.ConvTranspose2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        # output shape = (4, 64, 64)\n",
    "\n",
    "        self.conv5dec = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding='same')\n",
    "        # output shape = (4, 64, 64)\n",
    "        self.conv6dec = nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding='same')\n",
    "        # output shape = (4, 64, 64)\n",
    "        self.conv7dec = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=1, stride=1, padding='same')\n",
    "        # # End of Decoder # # # # # # # # # # # # # # # # # # # # \n",
    "        # output shape = (2, 64, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = F.relu(self.conv1enc(x))\n",
    "        x = h1 = F.relu(self.conv2enc(x))\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3enc(x))\n",
    "        x = h2 = F.relu(self.conv4enc(x))\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv5enc(x))\n",
    "        x = h3 = F.relu(self.conv6enc(x))\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = F.relu(self.conv7enc(x))\n",
    "        x = F.relu(self.conv8enc(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.upconv1(self.upsamp1(x))\n",
    "        x = torch.cat([h3, x], dim=1).contiguous()\n",
    "        x = F.relu(self.conv1dec(x))\n",
    "        x = F.relu(self.conv2dec(x))\n",
    "        \n",
    "        x = self.upconv2(self.upsamp2(x))\n",
    "        x = torch.cat([h2, x], dim=1).contiguous()\n",
    "        x = F.relu(self.conv3dec(x))\n",
    "        x = F.relu(self.conv4dec(x))\n",
    "        \n",
    "        x = self.upconv3(self.upsamp3(x))\n",
    "        x = torch.cat([h1, x], dim=1).contiguous()\n",
    "        x = F.relu(self.conv5dec(x))\n",
    "        x = F.relu(self.conv6dec(x))\n",
    "        x = self.conv7dec(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    \n",
    "    # Set network to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Iterate over dataset\n",
    "    losses = list()\n",
    "    for composite, target in data_loader:\n",
    "        # Move data to device\n",
    "        composite, target = composite.to(device), target.to(device)\n",
    "        \n",
    "        # Compute output\n",
    "        output = model(composite)\n",
    "        \n",
    "        # Compute crossentropy loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Track losses\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "    # Set network to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Return loss at end of epoch\n",
    "    return losses\n",
    "\n",
    "\n",
    "def eval_function(model, data_loader, criterion, device):\n",
    "    \n",
    "    mask_loss = 0.0  # loss for the entire dataset\n",
    "\n",
    "    # Set network to training mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over dataset\n",
    "        for composite, target in data_loader:\n",
    "            # Move data to device\n",
    "            composite, target = composite.to(device), target.to(device)\n",
    "\n",
    "            # Compute output\n",
    "            output = model(composite)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            mask_loss += loss.item() * composite.size(0)\n",
    "\n",
    "    return mask_loss / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Two of the main hyper-parameters are the batch-size and learning rate. We have already performed a small grid search and found the `batch_size = 256` of 256 and `lr=0.002` work well.  We use MSE as our loss-objective and will NOT use any regularization terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataloaders\n",
    "batch_size = 256  # the number of samples per batch\n",
    "# >>>>> YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line by your code.\")\n",
    "train_dataloader = ...  # the dataloader for the training dataset\n",
    "val_dataloader = ...  # the dataloader for the validation dataset\n",
    "# <<<<< END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model, optimizer and loss function\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "criterion = F.mse_loss\n",
    "print(model)\n",
    "print(f\"The model has {count_parameters(model):,} learnable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating the model\n",
    "losses = list()\n",
    "print(f\"Validation Mask loss before training: {eval_function(model, val_dataloader, criterion, device):.5f}\")\n",
    "for epoch in tqdm(range(5)):\n",
    "    epoch_losses = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "    losses.extend(epoch_losses)\n",
    "print(f\"Validation Mask loss after training: {eval_function(model, val_dataloader, criterion, device):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the first 5 images, target masks and predicted masks\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "fig, axs = plt.subplots(3, 5, figsize=(10, 6.5))\n",
    "with torch.no_grad():\n",
    "    composite, target = next(iter(val_dataloader))\n",
    "    output = model(composite).detach().cpu()\n",
    "    for i in range(5):\n",
    "        if i == 2:\n",
    "            axs[0, i].set_title(\"inputs\")\n",
    "            axs[1, i].set_title(\"targets\")\n",
    "            axs[2, i].set_title(\"predictions\")\n",
    "        axs[0, i].imshow(composite[i][0], cmap=\"gray\")\n",
    "        axs[0, i].axis(\"off\")\n",
    "        axs[1, i].imshow(target[i][0] - target[i][1], cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "        axs[1, i].axis(\"off\")\n",
    "        axs[2, i].imshow(output[i][0] - output[i][1], cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "        axs[2, i].axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
